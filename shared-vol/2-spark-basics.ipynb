{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Quicksand:300,700\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Fira Code\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"rise.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark basics\n",
    "\n",
    "![footer_logo_new](images/logo_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In this chapter we will cover the following topics:\n",
    "- Spark components\n",
    "- SparkSession\n",
    "- DataFrames\n",
    "- Transformation\n",
    "- Laziness\n",
    "- Actions\n",
    "- Lineage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark components\n",
    "\n",
    "Spark's execution environment is divided into the following components:\n",
    "\n",
    "- Driver\n",
    "    - Launches applications from outside or inside a cluster.\n",
    "- Executors\n",
    "    - Separate execution engines or containers on the worker nodes of a cluster.\n",
    "    - Tasks (unit of work) are run within the executors.\n",
    "- Cluster manager\n",
    "    - Allocates computing resources (CPU/Memory) in the distributed system the Spark application is run on.\n",
    "    - Examples are Yarn, Mesos, Spark, Kubernetes.\n",
    "- History Server\n",
    "    - Loads event logs from competed applications and show runtime details like: query plans, tasks, executors.\n",
    "    - Used to improve performance of the job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Spark execution model](images/spark-cluster.png \"Spark execution model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why the manager though?\n",
    "We just learned that Spark has a cluster manager, but don't we already have a driver?\n",
    "Let's go over some more history of how this came about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kubernetes\n",
    "In recent years, it's been hard to miss the rise of Kubernetes.\n",
    "Spark can run on Kubernetes too:\n",
    "\n",
    "![](images/spark-on-kubernetes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Running Spark on Kubernetes\n",
    "There are a few things to point out here:\n",
    "+ Data locality (i.e. moving the code to the data) tends to get lost on Kubernetes, as your storage generally is external to the Kubernetes cluster.\n",
    "+ The driver pod will stay available in the _completed_ state until Kubernetes decides to garbage collect it. While 'completed' no resources are used (but it does clutter your cluster).\n",
    "+ Spark 3.0 promises to be better optimized for Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SparkSession\n",
    "\n",
    "Main entrypoint for (new) Spark applications and a handle to the execution environment.\n",
    "\n",
    "```python\n",
    "spark = (\n",
    "    pyspark.sql.SparkSession.builder\n",
    "    .getOrCreate()\n",
    ")\n",
    "```\n",
    "The SparkSession provides builtin support for Hive features including  writing queries using HiveQL, access to Hive UDFs, and reading Hive tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"spark-2\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"/opt/workspace/history\").\\\n",
    "        enableHiveSupport().\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://34bf7bcebd56:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb3a6b1cc18>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='file:/opt/workspace/spark-warehouse')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intermezzo: Hive\n",
    "- Data warehouse infrastructure on top of Hadoop.\n",
    "- Imposes table structure and querying capabilities:\n",
    "    - Metastore: store metadata on tables (schema, location in HDFS etc.)\n",
    "    - HiveQL: query language, basically an SQL dialect\n",
    "- Originally it generated MapReduce jobs to run each query.\n",
    "- These days it generates and submits Spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Hive isn't necessarily a crazy choice for production jobs, even though people treat is as old-fashioned.\n",
    "It has a reputation for reliability. It also has quite a few sophisticated knobs to tweak for tuning the way data is layed out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrames\n",
    "\n",
    " - At its heart, we use Spark to manipulate large amounts of data.\n",
    " - The data structure (most often) used to do this is a DataFrame.\n",
    " - We can create a DataFrame in 3 ways:\n",
    " \n",
    "   1. Using existing data (for example: read from files).\n",
    "   2. Generating data in memory. (We'll see how to do this later.)\n",
    "   3. By transforming another DataFrame.   \n",
    "   \n",
    " - Dataframes are also immutable: the data they represent does not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transforming a DataFrame\n",
    "\n",
    "We have many many ways of transforming DFs to produce new ones. Some key transformations:\n",
    "\n",
    " - `select()`\n",
    " - `drop()`\n",
    " - `where()`\n",
    " - `groupby()`\n",
    " - `join()`\n",
    " - `distinct()`\n",
    " - `fillna()`\n",
    " \n",
    "The API documentation describes all these: there are many methods. We'll cover some of them later. Transformations are the ones that *return a DataFrame*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrames are Lazy\n",
    "\n",
    " - When we create a DataFrame, its content is *not* evaluated.\n",
    " - Instead a *lineage* is constructed: each DataFrame knows what parent DataFrame it depends on, and what it needs to do, but won't actually do anything until the content of the DataFrame is actually required.\n",
    " \n",
    "When is the content of the DataFrame actually required?\n",
    "\n",
    "In the next slide, the empty boxes mean *Unevaluated DataFrame here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![lineage](images/spark_df.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrames Actions\n",
    "\n",
    "- Actions trigger a DataFrame to evaluate its content, which is normally based on the lineage in a recursive manner.\n",
    "- These are normally methods on the DataFrame that _don't_ return a DataFrame.\n",
    "- Some examples include:\n",
    "    - `toPandas()`\n",
    "    - `count()`\n",
    "    - `collect()`\n",
    "    - `write()`\n",
    "- Note: some actions may be problematic to run due to memory issues. Can you identify which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here below we can see that, until we call `.count`, nothing gets evaluated. This applies to all actions:\n",
    "\n",
    "![lineage](images/spark_action.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What if things go south?\n",
    "\n",
    "Spark keeps the lineage of these various boxes (DataFrame) in the memory of the driver. If something goes wrong (hard drive crashes, memory, etc.) in the workers, the driver can rebuild the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this chapter we covered:\n",
    "\n",
    "- The execution model of Spark.\n",
    "- What the Spark context is and does.\n",
    "- DataFrame operations/transformation/actions/lineage."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
